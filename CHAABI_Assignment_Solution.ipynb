{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "96Pu3bY1r8pB"
      ],
      "gpuType": "T4",
      "mount_file_id": "1Ue3oFXa1bsTGhon4V-a8Zf75seykbS9t",
      "authorship_tag": "ABX9TyPzP/mVGiGa6n1pJV0UX6e1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushant-97/Chaabi_Assignment_CSV_Chat/blob/main/CHAABI_Assignment_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers qdrant_client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fU0rVgyMefn",
        "outputId": "c1e1cbce-ff2a-4a17-8bcb-b125ddef172b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qdrant_client\n",
            "  Downloading qdrant_client-1.6.9-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant_client) (1.59.2)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant_client)\n",
            "  Downloading grpcio_tools-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx[http2]>=0.14.0 (from qdrant_client)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0 (from qdrant_client)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant_client) (1.10.13)\n",
            "Collecting urllib3<2.0.0,>=1.26.14 (from qdrant_client)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<5.0dev,>=4.21.6 (from grpcio-tools>=1.41.0->qdrant_client)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.41.0 (from qdrant_client)\n",
            "  Downloading grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant_client) (67.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant_client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant_client) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx[http2]>=0.14.0->qdrant_client)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant_client) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant_client) (1.3.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.14.0->qdrant_client)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx[http2]>=0.14.0->qdrant_client)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]>=0.14.0->qdrant_client) (1.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=708a3d0ebdaed9728f9d43b7c1b2ff5bb7e68b804443bc716cda6cc03912994c\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, urllib3, protobuf, portalocker, hyperframe, hpack, h11, grpcio, httpcore, h2, grpcio-tools, httpx, qdrant_client, sentence_transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.59.2\n",
            "    Uninstalling grpcio-1.59.2:\n",
            "      Successfully uninstalled grpcio-1.59.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.59.3 grpcio-tools-1.59.3 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.2 httpx-0.25.2 hyperframe-6.0.1 portalocker-2.8.2 protobuf-4.25.1 qdrant_client-1.6.9 sentence_transformers-2.2.2 sentencepiece-0.1.99 urllib3-1.26.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkIxcISqXUZq",
        "outputId": "84dc4678-0fed-406c-d9ad-3419b2c375b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from sentence_transforxmers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "# Load your CSV file into a pandas DataFrame\n",
        "csv_file_path = '/content/drive/MyDrive/IITG/PLACEMENTS/Assignments/CHAABI/bigBasketProducts.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "df = df[:100]"
      ],
      "metadata": {
        "id": "mGV568hlmXcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inintal Trial"
      ],
      "metadata": {
        "id": "96Pu3bY1r8pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.iloc[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtdIhrHhYcb3",
        "outputId": "cbda40f8-18f1-46ee-a1d4-47809d10ea88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[3].to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpCe6eKMYlEn",
        "outputId": "f01d982b-d9dc-4d0f-916d-75d7be9ffa01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'index': 4,\n",
              " 'product': 'Cereal Flip Lid Container/Storage Jar - Assorted Colour',\n",
              " 'category': 'Cleaning & Household',\n",
              " 'sub_category': 'Bins & Bathroom Ware',\n",
              " 'brand': 'Nakoda',\n",
              " 'sale_price': 149.0,\n",
              " 'market_price': 176.0,\n",
              " 'type': 'Laundry, Storage Baskets',\n",
              " 'rating': 3.7,\n",
              " 'description': 'Multipurpose container with an attractive design and made from food-grade plastic for your hygiene and safety ideal for storing pulses. Grains, spices, and more with easy opening and closing flip-open lid. Strong, durable and transparent body for longevity and easy identification of contents. Multipurpose storage solution for your daily needs stores your everyday food essentials in style with the Nakoda container set. With transparent bodies, you can easily identify your stored items without having to open the lids. These containers are ideal for storing a large variety of items such as food grains, snacks and pulses to sugar, spices, condiments and more. Featuring unique flip-open lids, you can easily open and close this container without any hassles.\\nThe Nakoda container is made from high-quality food-grade and BPA-free plastic that is 100% safe for storing food items. You can safely store your food items in this container without worrying about contamination and harmful toxins. As they are constructed using highly durable virgin plastic, this container will last for a long time even with regular use. This container can enhance the overall look of your kitchen decor. Being dishwasher safe, cleaning and maintaining this container is an easy task. You can also use a simple soap solution to manually wash and retain their looks for a long time.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained model for generating embeddings\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for each row in the DataFrame\n",
        "embeddings = model.encode(df['description'].astype(str), convert_to_tensor=False)\n",
        "\n",
        "# Convert embeddings to PyTorch tensors\n",
        "embeddings = torch.tensor(embeddings, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "xS0ODSvBmau9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "894e38fc-e8ef-434d-a6b3-74532851521a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d07ca3f4fd93>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load a pre-trained model for generating embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'paraphrase-MiniLM-L6-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Generate embeddings for each row in the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1tDvpdvo4mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d36927e-f648-4ce0-fa4f-5e13c80a2fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 0.2139\n",
            "Epoch [20/50], Loss: 0.0728\n",
            "Epoch [30/50], Loss: 0.0554\n",
            "Epoch [40/50], Loss: 0.0536\n",
            "Epoch [50/50], Loss: 0.0528\n"
          ]
        }
      ],
      "source": [
        "# Define the autoencoder model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Set the input and encoding dimensions\n",
        "input_dim = len(embeddings[0])\n",
        "encoding_dim = 200\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Autoencoder(input_dim, encoding_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "# Train the autoencoder\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    outputs = autoencoder(embeddings)\n",
        "    loss = criterion(outputs, embeddings)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Get the reduced embeddings\n",
        "with torch.no_grad():\n",
        "    reduced_embeddings = autoencoder.encoder(embeddings).numpy()\n",
        "\n",
        "# Convert the reduced embeddings to a list of lists\n",
        "embedding_list = reduced_embeddings.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7oomGOiotuu",
        "outputId": "28185e32-9af8-4ee5-a0ea-c35397a5d6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import models, QdrantClient"
      ],
      "metadata": {
        "id": "Ymk7XdeWoAXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant = QdrantClient(\":memory:\")"
      ],
      "metadata": {
        "id": "tDjCIXwrlhc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_=models.VectorParams(\n",
        "        size=200,  # Vector size is defined by used model\n",
        "        distance=models.Distance.COSINE,\n",
        "    )"
      ],
      "metadata": {
        "id": "st7vssY6oPeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant.recreate_collection(\n",
        "    collection_name=\"test_collection\",\n",
        "    vectors_config=vectors_,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxUcihlen4UJ",
        "outputId": "9fc0f7ae-909a-4e38-dee7-36dccc00c0be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# records = [x for idx, doc in enumerate(embedding_list)]"
      ],
      "metadata": {
        "id": "FfizG81romnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# records=[models.Record(id=idx, vector=encoder.encode(doc[\"descri ption\"]).tolist(), payload=doc) for idx, doc in enumerate(documents)]"
      ],
      "metadata": {
        "id": "GiSKl3svodu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qdrant.upload_records(\n",
        "#     collection_name=\"test_collection\",\n",
        "#     records=[\n",
        "#         models.Record(\n",
        "#             id=idx, vector=encoder.encode(doc[\"description\"]).tolist(), payload=doc\n",
        "#         )\n",
        "#         for idx, doc in enumerate(documents)\n",
        "#     ],\n",
        "# )"
      ],
      "metadata": {
        "id": "cmtdgSn_oX9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client.http.models import PointStruct\n",
        "\n",
        "vec = [PointStruct(id = idx, vector = x, payload = df.iloc[idx]) for idx,x in enumerate(embedding_list)]\n",
        "\n",
        "operation_info = qdrant.upsert(\n",
        "    collection_name=\"test_collection\",\n",
        "    wait=True,\n",
        "    points=vec,\n",
        ")\n",
        "\n",
        "print(operation_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZr3q75mm8A9",
        "outputId": "73df71c1-438b-4631-d83f-7a2ba2c74394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "operation_id=0 status=<UpdateStatus.COMPLETED: 'completed'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKjVG5jrsD9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_result = qdrant.search(\n",
        "    collection_name=\"test_collection\", query_vector=embedding_list[4], limit=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "_qW4cw0Zr4mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(search_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd2IkALysKfa",
        "outputId": "9b899255-7ea7-43b9-aecf-9bcf9b70d8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_result(search_result):\n",
        "  # Print information for each scored point\n",
        "  for point in search_result:\n",
        "      print(f\"Product ID: {point.id}\")\n",
        "      print(f\"Score: {point.score}\")\n",
        "\n",
        "      # Print payload details\n",
        "      payload = point.payload\n",
        "      print(f\"Product Name: {payload['product']}\")\n",
        "      print(f\"Category: {payload['category']}\")\n",
        "      print(f\"Sub-category: {payload['sub_category']}\")\n",
        "      print(f\"Brand: {payload['brand']}\")\n",
        "      print(f\"Sale Price: {payload['sale_price']}\")\n",
        "      print(f\"Market Price: {payload['market_price']}\")\n",
        "      print(f\"Type: {payload['type']}\")\n",
        "      print(f\"Rating: {payload['rating']}\")\n",
        "      print(f\"Description: {payload['description']}\")\n",
        "\n",
        "      print(\"\\n----------------------\\n\")\n"
      ],
      "metadata": {
        "id": "w9mJfEqJseMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "QqX7Y3zyuzaR",
        "outputId": "4301677e-5fe4-490f-e73c-b4cc7951f4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                            product  \\\n",
              "0      1             Garlic Oil - Vegetarian Capsule 500 mg   \n",
              "1      2                              Water Bottle - Orange   \n",
              "2      3                     Brass Angle Deep - Plain, No.2   \n",
              "3      4  Cereal Flip Lid Container/Storage Jar - Assort...   \n",
              "4      5                 Creme Soft Soap - For Hands & Body   \n",
              "\n",
              "                 category           sub_category              brand  \\\n",
              "0        Beauty & Hygiene              Hair Care  Sri Sri Ayurveda    \n",
              "1  Kitchen, Garden & Pets  Storage & Accessories         Mastercook   \n",
              "2    Cleaning & Household            Pooja Needs                Trm   \n",
              "3    Cleaning & Household   Bins & Bathroom Ware             Nakoda   \n",
              "4        Beauty & Hygiene       Bath & Hand Wash              Nivea   \n",
              "\n",
              "   sale_price  market_price                      type  rating  \\\n",
              "0       220.0         220.0          Hair Oil & Serum     4.1   \n",
              "1       180.0         180.0    Water & Fridge Bottles     2.3   \n",
              "2       119.0         250.0           Lamp & Lamp Oil     3.4   \n",
              "3       149.0         176.0  Laundry, Storage Baskets     3.7   \n",
              "4       162.0         162.0      Bathing Bars & Soaps     4.4   \n",
              "\n",
              "                                         description  \n",
              "0  This Product contains Garlic Oil that is known...  \n",
              "1  Each product is microwave safe (without lid), ...  \n",
              "2  A perfect gift for all occasions, be it your m...  \n",
              "3  Multipurpose container with an attractive desi...  \n",
              "4  Nivea Creme Soft Soap gives your skin the best...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e9f6b5d-a8ef-459d-aa2e-78385c862cfe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>product</th>\n",
              "      <th>category</th>\n",
              "      <th>sub_category</th>\n",
              "      <th>brand</th>\n",
              "      <th>sale_price</th>\n",
              "      <th>market_price</th>\n",
              "      <th>type</th>\n",
              "      <th>rating</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Garlic Oil - Vegetarian Capsule 500 mg</td>\n",
              "      <td>Beauty &amp; Hygiene</td>\n",
              "      <td>Hair Care</td>\n",
              "      <td>Sri Sri Ayurveda</td>\n",
              "      <td>220.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>Hair Oil &amp; Serum</td>\n",
              "      <td>4.1</td>\n",
              "      <td>This Product contains Garlic Oil that is known...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Water Bottle - Orange</td>\n",
              "      <td>Kitchen, Garden &amp; Pets</td>\n",
              "      <td>Storage &amp; Accessories</td>\n",
              "      <td>Mastercook</td>\n",
              "      <td>180.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>Water &amp; Fridge Bottles</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Each product is microwave safe (without lid), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Brass Angle Deep - Plain, No.2</td>\n",
              "      <td>Cleaning &amp; Household</td>\n",
              "      <td>Pooja Needs</td>\n",
              "      <td>Trm</td>\n",
              "      <td>119.0</td>\n",
              "      <td>250.0</td>\n",
              "      <td>Lamp &amp; Lamp Oil</td>\n",
              "      <td>3.4</td>\n",
              "      <td>A perfect gift for all occasions, be it your m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Cereal Flip Lid Container/Storage Jar - Assort...</td>\n",
              "      <td>Cleaning &amp; Household</td>\n",
              "      <td>Bins &amp; Bathroom Ware</td>\n",
              "      <td>Nakoda</td>\n",
              "      <td>149.0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>Laundry, Storage Baskets</td>\n",
              "      <td>3.7</td>\n",
              "      <td>Multipurpose container with an attractive desi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Creme Soft Soap - For Hands &amp; Body</td>\n",
              "      <td>Beauty &amp; Hygiene</td>\n",
              "      <td>Bath &amp; Hand Wash</td>\n",
              "      <td>Nivea</td>\n",
              "      <td>162.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Bathing Bars &amp; Soaps</td>\n",
              "      <td>4.4</td>\n",
              "      <td>Nivea Creme Soft Soap gives your skin the best...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e9f6b5d-a8ef-459d-aa2e-78385c862cfe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e9f6b5d-a8ef-459d-aa2e-78385c862cfe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e9f6b5d-a8ef-459d-aa2e-78385c862cfe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c7e03a69-2cce-430e-aed5-3553e3545c17\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7e03a69-2cce-430e-aed5-3553e3545c17')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c7e03a69-2cce-430e-aed5-3553e3545c17 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqi3DZ19v_Wr",
        "outputId": "ac17c0b2-79e2-43f9-8c96-748a66566e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1945823431015015"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "query = \"How many products are of type Hair Oil and Serum?\"\n",
        "query_vec_model = model.encode(query, convert_to_tensor=False)\n",
        "# Convert embeddings to PyTorch tensors\n",
        "query_vec_model = torch.tensor(query_vec_model, dtype=torch.float32)\n",
        "\n",
        "# Get the reduced embeddings\n",
        "with torch.no_grad():\n",
        "    reduced_embeddings = autoencoder.encoder(query_vec_model).numpy()\n",
        "\n",
        "# Convert the reduced embeddings to a list of lists\n",
        "query_vector_ = reduced_embeddings.tolist()"
      ],
      "metadata": {
        "id": "YjR16eYVux1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector_"
      ],
      "metadata": {
        "id": "x4tGwPadwGOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26768bf6-462f-49e0-e82a-5b2f24c79ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3099710941314697,\n",
              " 0.7554594874382019,\n",
              " 0.35983678698539734,\n",
              " 0.8897739052772522,\n",
              " 1.092047095298767,\n",
              " 0.48170894384384155,\n",
              " 0.8669180870056152,\n",
              " 1.371763825416565,\n",
              " 1.3771411180496216,\n",
              " 0.9507513046264648,\n",
              " 1.329007863998413,\n",
              " 1.3444126844406128,\n",
              " 1.04909086227417,\n",
              " 1.2602484226226807,\n",
              " 1.4953609704971313,\n",
              " 1.4163143634796143,\n",
              " 1.5264003276824951,\n",
              " 1.258849024772644,\n",
              " 0.9477965235710144,\n",
              " 0.808463454246521,\n",
              " 1.435517430305481,\n",
              " 1.3615449666976929,\n",
              " 0.9383190274238586,\n",
              " 1.2793042659759521,\n",
              " 1.26472806930542,\n",
              " 1.3350491523742676,\n",
              " 0.0,\n",
              " 0.9604185223579407,\n",
              " 1.314059853553772,\n",
              " 1.3818464279174805,\n",
              " 1.5091183185577393,\n",
              " 1.0339182615280151,\n",
              " 1.3749394416809082,\n",
              " 1.2502436637878418,\n",
              " 1.0954664945602417,\n",
              " 0.9696476459503174,\n",
              " 0.0,\n",
              " 1.2742242813110352,\n",
              " 0.7824317812919617,\n",
              " 0.8719455599784851,\n",
              " 1.565778374671936,\n",
              " 1.1682097911834717,\n",
              " 0.0,\n",
              " 1.4655669927597046,\n",
              " 1.3645809888839722,\n",
              " 0.8503220677375793,\n",
              " 1.3474467992782593,\n",
              " 0.9558589458465576,\n",
              " 1.3721716403961182,\n",
              " 0.41091814637184143,\n",
              " 0.0,\n",
              " 0.8281488418579102,\n",
              " 1.3117629289627075,\n",
              " 0.8432536125183105,\n",
              " 1.1695895195007324,\n",
              " 1.3336526155471802,\n",
              " 0.7728562355041504,\n",
              " 1.032395362854004,\n",
              " 0.0,\n",
              " 1.015425205230713,\n",
              " 1.423746109008789,\n",
              " 1.041856288909912,\n",
              " 1.3062981367111206,\n",
              " 1.2559313774108887,\n",
              " 0.6584715843200684,\n",
              " 0.25477495789527893,\n",
              " 0.9464443325996399,\n",
              " 0.0,\n",
              " 0.8459509611129761,\n",
              " 1.0917772054672241,\n",
              " 0.8699243068695068,\n",
              " 1.507258653640747,\n",
              " 0.9612549543380737,\n",
              " 1.0994359254837036,\n",
              " 1.0487079620361328,\n",
              " 1.1184896230697632,\n",
              " 1.2120509147644043,\n",
              " 0.0,\n",
              " 1.2284172773361206,\n",
              " 0.5581458806991577,\n",
              " 1.0671573877334595,\n",
              " 0.0,\n",
              " 1.168097734451294,\n",
              " 1.199040412902832,\n",
              " 1.240136981010437,\n",
              " 1.315028190612793,\n",
              " 1.0715057849884033,\n",
              " 1.2890821695327759,\n",
              " 1.5534627437591553,\n",
              " 1.3443492650985718,\n",
              " 1.2181340456008911,\n",
              " 1.0221943855285645,\n",
              " 1.0931837558746338,\n",
              " 0.030946936458349228,\n",
              " 0.0,\n",
              " 1.9726356267929077,\n",
              " 1.6554509401321411,\n",
              " 1.0386171340942383,\n",
              " 1.1171821355819702,\n",
              " 0.6013807654380798,\n",
              " 0.8656148314476013,\n",
              " 0.9913640022277832,\n",
              " 0.9757599830627441,\n",
              " 1.0500091314315796,\n",
              " 1.0859001874923706,\n",
              " 0.34650126099586487,\n",
              " 1.048898696899414,\n",
              " 1.221679925918579,\n",
              " 1.2585548162460327,\n",
              " 1.0812197923660278,\n",
              " 1.0859485864639282,\n",
              " 0.24037766456604004,\n",
              " 0.0,\n",
              " 0.5429831147193909,\n",
              " 0.8956837058067322,\n",
              " 0.8789443969726562,\n",
              " 0.9450978636741638,\n",
              " 1.2253296375274658,\n",
              " 1.1063154935836792,\n",
              " 1.5065462589263916,\n",
              " 0.86493319272995,\n",
              " 1.1333742141723633,\n",
              " 1.5956051349639893,\n",
              " 1.44277822971344,\n",
              " 0.9590766429901123,\n",
              " 0.13248784840106964,\n",
              " 1.2489315271377563,\n",
              " 0.06481967866420746,\n",
              " 0.10455882549285889,\n",
              " 1.1495906114578247,\n",
              " 1.0424187183380127,\n",
              " 1.0942224264144897,\n",
              " 1.4715207815170288,\n",
              " 0.37113767862319946,\n",
              " 1.0797852277755737,\n",
              " 1.2531800270080566,\n",
              " 1.3827838897705078,\n",
              " 1.04275643825531,\n",
              " 1.5415009260177612,\n",
              " 1.0259705781936646,\n",
              " 1.1549434661865234,\n",
              " 1.0595091581344604,\n",
              " 1.3683500289916992,\n",
              " 1.2783000469207764,\n",
              " 0.7414718270301819,\n",
              " 1.6517504453659058,\n",
              " 1.312300682067871,\n",
              " 1.138307809829712,\n",
              " 1.3233755826950073,\n",
              " 0.9409207105636597,\n",
              " 0.4248535931110382,\n",
              " 1.1484036445617676,\n",
              " 0.9125438928604126,\n",
              " 1.1202235221862793,\n",
              " 0.0,\n",
              " 1.0714995861053467,\n",
              " 1.2615153789520264,\n",
              " 1.1784590482711792,\n",
              " 1.4702537059783936,\n",
              " 0.0,\n",
              " 0.26313328742980957,\n",
              " 0.45880255103111267,\n",
              " 0.9712641835212708,\n",
              " 0.0,\n",
              " 1.1515194177627563,\n",
              " 1.3480541706085205,\n",
              " 0.49755215644836426,\n",
              " 1.222273349761963,\n",
              " 0.7917035818099976,\n",
              " 1.004385232925415,\n",
              " 1.1461817026138306,\n",
              " 1.5591555833816528,\n",
              " 1.3890702724456787,\n",
              " 1.0892845392227173,\n",
              " 1.2032057046890259,\n",
              " 1.797049641609192,\n",
              " 1.2614729404449463,\n",
              " 1.299330472946167,\n",
              " 0.0,\n",
              " 0.7773681282997131,\n",
              " 1.0452800989151,\n",
              " 1.180957555770874,\n",
              " 1.0798996686935425,\n",
              " 1.1119428873062134,\n",
              " 1.0343914031982422,\n",
              " 0.7094680070877075,\n",
              " 0.9032476544380188,\n",
              " 0.850659966468811,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.38717159628868103,\n",
              " 1.2710176706314087,\n",
              " 1.3545163869857788,\n",
              " 1.4682111740112305,\n",
              " 1.1781272888183594,\n",
              " 1.093085527420044,\n",
              " 1.567294955253601,\n",
              " 1.0875680446624756,\n",
              " 1.3201375007629395,\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_result = qdrant.search(\n",
        "    collection_name=\"test_collection\", query_vector=query_vector_, limit=3\n",
        ")"
      ],
      "metadata": {
        "id": "bXOcJ_Afvc9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_result(search_result)"
      ],
      "metadata": {
        "id": "mFCh2h6bwN_3",
        "outputId": "2c527f57-7447-4e23-9b4a-8521b39c0237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product ID: 16\n",
            "Score: 0.987340532957319\n",
            "Product Name: Smooth Skin Oil - For Dry Skin\n",
            "Category: Beauty & Hygiene\n",
            "Sub-category: Skin Care\n",
            "Brand: Aroma Treasures\n",
            "Sale Price: 324.0\n",
            "Market Price: 360.0\n",
            "Type: Aromatherapy\n",
            "Rating: 5.0\n",
            "Description: Specially crafted for dry skin, this richly formulated skin oil restores the natural moisture of the skin, improves its health and youthfulness. Not to mention how soft, shiny and smooth it makes your skin.\r\n",
            "\r\n",
            "\r\n",
            "Aroma Treasures, pioneers of aromatherapy have put in years of research and study to give you the ultimate range of beauty care products.\n",
            "\n",
            "----------------------\n",
            "\n",
            "Product ID: 57\n",
            "Score: 0.9869918009899178\n",
            "Product Name: Argan-Liquid Gold Hair Spa\n",
            "Category: Beauty & Hygiene\n",
            "Sub-category: Hair Care\n",
            "Brand: Aroma Treasures\n",
            "Sale Price: 199.5\n",
            "Market Price: 210.0\n",
            "Type: Hair & Scalp Treatment\n",
            "Rating: 4.4\n",
            "Description: Our beautifully crafted Hair Spa Collection promises to be the perfect solution for nourishing and rejuvenating your tresses. Made with Argan Oil it revives moisture in the dull dry hair, reduces frizz, smooths unmanageable locks of curly hair, deeply nurtures hair shafts & follicles to prevent hair fall, repair split ends and strengthens the roots!Revitalising Hair Oil - The luxurious fusion of Moroccan Argan Oil, Sesame Oil, Olive Oil, Lavender and Geranium provides intense nutrition to revitalize dry and damaged hair.Follikare Wash - Precious blend of Moroccan Argan Oil, Hibiscus, and Ylang Ylang nudges away sebum to strengthen the hair follicles and improve hair growth.Follikare Deep Conditioning Mask - Perfect balance of Moroccan Argan Oil, Patchouli and Shea Butter restores hairs natural luster and provide deep nourishment to brittle, weak and frizzy locks.Restorative Nourishing Tonic - Exotic elixir of Moroccan Argan Oil, Chamomile, Watercress and Sage Extracts strengthens the root to prevent hair fall, enhance the volume and silken the texture of hair.Satin Feel Leave-On - Potent serum of Moroccan Argan Oil and Vitamin E repairs damaged hair strands, while moisturises and smoothens unmanageable dry frizzy hair.Argan - Liquid Gold - Aroma Treasures brings to you an exclusive Hair Spa Collection enriched with Argan Oil extracted from the kernels of Moroccos Argan tree.\n",
            "\n",
            "----------------------\n",
            "\n",
            "Product ID: 8\n",
            "Score: 0.9868053924197339\n",
            "Product Name: Biotin & Collagen Volumizing Hair Shampoo + Biotin & Collagen Hair Conditioner\n",
            "Category: Beauty & Hygiene\n",
            "Sub-category: Hair Care\n",
            "Brand: StBotanica\n",
            "Sale Price: 1098.0\n",
            "Market Price: 1098.0\n",
            "Type: Shampoo & Conditioner\n",
            "Rating: 3.5\n",
            "Description: An exclusive blend with Vitamin B7 Biotin, Hydrolyzed collagen, Oat Extract along with premium & organic cold-pressed ingredients helps to infuse nutrients into every strand and creates the appearance of thicker, fuller healthier looking hair. This powerful formula helps volumize even the skinniest strands into fuller and more abundant looking locks. It is safe for color-treated hair and safe for all hair types. St Botanica Biotin & Collagen Hair Conditioner has been specially formulated to repair dry & damaged hair for full, thick, voluminous, shiny & healthy looking hair! The amazing hair conditioner ingredients include Biotin, Hydrolyzed Collagen, Pro-Vitamin B5, Vitamin E, & Hydrolyzed Silk Proteins for glistening looking hair. Biotin and Collagen, infused with most efficacious natural extracts not only promotes healthy hair growth but also prevents hair dryness, increases the elasticity of the hair cortex, thereby strengthening hair, minimizing hair breakage and helping hair grow longer, healthier and thicker. PLUMP IT UP: the nutrient-rich, plump-it-up power of this haircare infused with ProVitamin B7 biotin and collagen helps give each strand of hair a beautiful boost, this dynamic duo will leave your hair feeling thicker, fuller, and looking oh, so healthy.\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_result1 = qdrant.search(\n",
        "    collection_name=\"test_collection\", query_vector=embedding_list[4], limit=3\n",
        ")\n",
        "\n",
        "print_result(search_result1)\n",
        "print(\"***************\")\n",
        "\n",
        "search_result2 = qdrant.search(\n",
        "    collection_name=\"test_collection\", query_vector=embedding_list[42], limit=3\n",
        ")\n",
        "print_result(search_result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY8lAOdwsxY2",
        "outputId": "ffa9840e-e64f-4115-d8f1-261d211da415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product ID: 4\n",
            "Score: 1.0000000055017462\n",
            "Product Name: Creme Soft Soap - For Hands & Body\n",
            "Category: Beauty & Hygiene\n",
            "Sub-category: Bath & Hand Wash\n",
            "Brand: Nivea\n",
            "Sale Price: 162.0\n",
            "Market Price: 162.0\n",
            "Type: Bathing Bars & Soaps\n",
            "Rating: 4.4\n",
            "Description: Nivea Creme Soft Soap gives your skin the best care that it must get. The soft bar consists of Vitamins F and Almonds which are really skin gracious and help you get great skin. It provides the skin with moisture and leaves behind flawless and smooth skin. It makes sure that your body is totally free of germs & dirt and at the same time well nourished.For Beauty tips, tricks & more visit https://bigbasket.blog/\n",
            "\n",
            "----------------------\n",
            "\n",
            "Product ID: 42\n",
            "Score: 0.9936772142141496\n",
            "Product Name: Flavoured Cream Wafer Roll - Strawberry\n",
            "Category: Gourmet & World Food\n",
            "Sub-category: Chocolates & Biscuits\n",
            "Brand: Twister\n",
            "Sale Price: 275.0\n",
            "Market Price: 275.0\n",
            "Type: Cookies, Biscotti, Wafer\n",
            "Rating: 4.3\n",
            "Description: Very crispiest texture and pleasant odour of flavour.\n",
            "\n",
            "----------------------\n",
            "\n",
            "Product ID: 71\n",
            "Score: 0.9933081321241697\n",
            "Product Name: Pure & Gentle Face Wash\n",
            "Category: Beauty & Hygiene\n",
            "Sub-category: Skin Care\n",
            "Brand: Pears\n",
            "Sale Price: 120.0\n",
            "Market Price: 150.0\n",
            "Type: Face Care\n",
            "Rating: 4.3\n",
            "Description: Pears is the gentle way to keep your skin looking innocent and beautiful. It is enriched with pure glycerin and natural oils that gently moisturize skin to keep it smooth while its mild fragrance and soft lather ensure that your skin gets the pampering it deserves. Its natural glycerin provides moisture to your skin, making your face feel silky smooth while it stays clear and radiant! . Enriched with pure glycerine and natural oils that gently moisturize skin to keep it smooth. Mild fragrance and soft lather ensure that skin gets a well deserved pampering. Soap free washing preparation Pears Brand: With the goodness of glycerin & natural oils, Pears is trusted for being gentle, and is recommended by doctors and pediatricians worldwide. It keeps your skin soft and smiling with innocence. It is so pure that you can actually see through it! The pure and gentle face wash gives you fresh and clear skin in a gentle way. The face wash is suitable for frequent use as it has a soap-free formulation. Its mild fragrance will leave you feeling fresh and clean and this gentle wash not only cleans your skin but also rejuvenates it from deep within. This face wash will not leave your skin dry as it is enriched with glycerin and natural oils. So pamper your skin and nourish it at the same time with this mild face wash from Pears. You can carry this tube even in your bag, get this face wash right away!\r\r\r\n",
            "\r\r\r\n",
            "Features and Benefits:\r\r\r\n",
            "• Enriched with pure glycerin that gently moisturize the skin for a smooth feel.\r\r\r\n",
            "• Deeply cleanse & wash away dead skin cells.\r\r\r\n",
            "• This provides you a soap-free washing preparation.\r\r\r\n",
            "• soap-free formulation.\r\r\r\n",
            "• Fresh and renewed skin.\r\r\r\n",
            "• Use twice a day for best results  For Beauty tips, tricks & more visitÃƒ€šÃ‚ https://bigbasket.blog/\n",
            "\n",
            "----------------------\n",
            "\n",
            "***************\n",
            "Product ID: 42\n",
            "Score: 0.9999999990617985\n",
            "Product Name: Flavoured Cream Wafer Roll - Strawberry\n",
            "Category: Gourmet & World Food\n",
            "Sub-category: Chocolates & Biscuits\n",
            "Brand: Twister\n",
            "Sale Price: 275.0\n",
            "Market Price: 275.0\n",
            "Type: Cookies, Biscotti, Wafer\n",
            "Rating: 4.3\n",
            "Description: Very crispiest texture and pleasant odour of flavour.\n",
            "\n",
            "----------------------\n",
            "\n",
            "Product ID: 4\n",
            "Score: 0.993677221228731\n",
            "Product Name: Creme Soft Soap - For Hands & Body\n",
            "Category: Beauty & Hygiene\n",
            "Sub-category: Bath & Hand Wash\n",
            "Brand: Nivea\n",
            "Sale Price: 162.0\n",
            "Market Price: 162.0\n",
            "Type: Bathing Bars & Soaps\n",
            "Rating: 4.4\n",
            "Description: Nivea Creme Soft Soap gives your skin the best care that it must get. The soft bar consists of Vitamins F and Almonds which are really skin gracious and help you get great skin. It provides the skin with moisture and leaves behind flawless and smooth skin. It makes sure that your body is totally free of germs & dirt and at the same time well nourished.For Beauty tips, tricks & more visit https://bigbasket.blog/\n",
            "\n",
            "----------------------\n",
            "\n",
            "Product ID: 78\n",
            "Score: 0.9917162157071694\n",
            "Product Name: Amla Chatpata Dry Fruit\n",
            "Category: Gourmet & World Food\n",
            "Sub-category: Snacks, Dry Fruits, Nuts\n",
            "Brand: Fresho Signature\n",
            "Sale Price: 49.0\n",
            "Market Price: 65.0\n",
            "Type: Dry Fruits & Berries\n",
            "Rating: nan\n",
            "Description: Fresho Signature presents an array of dry fruits apart from the ordinary to give you the best of the best to indulge. Want to try something other than a regular fruit? This sweetened and spiced amla dry fruit is a healthy alternative to munch on when hunger strikes. It contains amla fruit, sulphurless sugar, cumin seed, black pepper, black salt, citric acid, cinnamon, asafoetida and ginger powder. Easy to carry, it makes a great snack to keep you going throughout the day.\r\n",
            "\r\n",
            "Our spicy candied dry fruit is here to be your snack buddy and your healthy sugar high. They are sweetened with sulphurless sugar and liquid glucose and spiced to enhance the zing. There are 0 added preservatives, artificial colour and flavour to retain the original fruit essence and keep it natural and healthy.\n",
            "\n",
            "----------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Hm6mceXtE1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input from the command line\n",
        "user_input = input(\"Enter something: \")\n",
        "\n",
        "# Print the user input\n",
        "print(\"You entered:\", user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx3gnqpuo66H",
        "outputId": "390cf718-67f8-418a-b870-c8602752efe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter something: how are u?\n",
            "You entered: how are u?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqF68Cvpo7c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Run"
      ],
      "metadata": {
        "id": "lqyl6px4r_6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clone github repo\n",
        "!git clone \"https://github.com/sushant-97/Chaabi_Assignment_CSV_Chat.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzKUv97GsAr4",
        "outputId": "b11b1207-26d3-4fce-ea6c-e1f6ad10552d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chaabi_Assignment_CSV_Chat'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 17 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (17/17), 5.10 KiB | 5.10 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/Chaabi_Assignment_CSV_Chat/COLAB\")"
      ],
      "metadata": {
        "id": "oXC5z50hsI33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python all.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_SpLMNmsQYr",
        "outputId": "6e223313-7ab6-4049-c284-07fb6dea40da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "operation_id=0 status=<UpdateStatus.COMPLETED: 'completed'>\n",
            "tokenizer_config.json: 100% 287/287 [00:00<00:00, 1.26MB/s]\n",
            "tokenizer.json: 100% 2.73M/2.73M [00:00<00:00, 61.7MB/s]\n",
            "special_tokens_map.json: 100% 281/281 [00:00<00:00, 1.62MB/s]\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 4.17MB/s]\n",
            "configuration_falcon.py: 100% 7.16k/7.16k [00:00<00:00, 27.8MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
            "- configuration_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n",
            "modeling_falcon.py: 100% 56.9k/56.9k [00:00<00:00, 117MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
            "- modeling_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "pytorch_model.bin.index.json: 100% 16.9k/16.9k [00:00<00:00, 61.8MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 10.5M/9.95G [00:02<33:56, 4.88MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 21.0M/9.95G [00:02<20:36, 8.03MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 31.5M/9.95G [00:03<18:03, 9.15MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 41.9M/9.95G [00:04<15:31, 10.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 52.4M/9.95G [00:05<13:59, 11.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 62.9M/9.95G [00:06<14:20, 11.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 73.4M/9.95G [00:06<13:20, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.95G [00:07<12:40, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 94.4M/9.95G [00:08<12:14, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 105M/9.95G [00:09<11:54, 13.8MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 115M/9.95G [00:09<11:42, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 126M/9.95G [00:10<11:33, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 136M/9.95G [00:11<11:25, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.95G [00:11<11:23, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 157M/9.95G [00:12<11:16, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 168M/9.95G [00:13<12:22, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 178M/9.95G [00:14<13:05, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 189M/9.95G [00:15<13:31, 12.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 199M/9.95G [00:16<12:50, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.95G [00:17<13:21, 12.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 220M/9.95G [00:18<13:42, 11.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 231M/9.95G [00:18<12:57, 12.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 241M/9.95G [00:19<12:24, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 252M/9.95G [00:20<12:58, 12.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 262M/9.95G [00:21<12:28, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.95G [00:22<12:01, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 283M/9.95G [00:22<12:47, 12.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 294M/9.95G [00:23<12:15, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 304M/9.95G [00:24<11:53, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 315M/9.95G [00:25<12:34, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 325M/9.95G [00:26<12:13, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.95G [00:26<11:50, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 346M/9.95G [00:27<12:19, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 357M/9.95G [00:28<12:10, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 367M/9.95G [00:29<11:49, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 377M/9.95G [00:29<11:33, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 388M/9.95G [00:30<11:19, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/9.95G [00:31<11:13, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 409M/9.95G [00:32<11:49, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 419M/9.95G [00:32<11:49, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 430M/9.95G [00:33<11:11, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 440M/9.95G [00:34<10:23, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 451M/9.95G [00:34<10:30, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/9.95G [00:35<10:36, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 472M/9.95G [00:36<09:38, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 482M/9.95G [00:36<09:59, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 493M/9.95G [00:37<10:12, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 503M/9.95G [00:38<09:20, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 514M/9.95G [00:38<09:44, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/9.95G [00:39<10:02, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 535M/9.95G [00:39<09:13, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 545M/9.95G [00:40<09:40, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 556M/9.95G [00:41<09:59, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 566M/9.95G [00:42<10:10, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 577M/9.95G [00:42<09:19, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.95G [00:43<09:42, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 598M/9.95G [00:43<09:06, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 608M/9.95G [00:44<09:26, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 619M/9.95G [00:45<09:45, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 629M/9.95G [00:45<10:00, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 640M/9.95G [00:46<09:15, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 650M/9.95G [00:47<09:38, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 661M/9.95G [00:47<09:49, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 671M/9.95G [00:48<09:05, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 682M/9.95G [00:49<09:28, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 692M/9.95G [00:49<08:53, 17.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 703M/9.95G [00:50<09:15, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 713M/9.95G [00:51<09:37, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 724M/9.95G [00:51<09:52, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 734M/9.95G [00:52<09:03, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 744M/9.95G [00:52<09:29, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 755M/9.95G [00:53<09:45, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 765M/9.95G [00:54<08:58, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 776M/9.95G [00:54<09:23, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 786M/9.95G [00:55<08:43, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 797M/9.95G [00:56<09:12, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 807M/9.95G [00:56<09:33, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 818M/9.95G [00:57<08:49, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 828M/9.95G [00:57<09:16, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 839M/9.95G [00:58<09:35, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 849M/9.95G [00:59<09:50, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 860M/9.95G [01:00<09:59, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 870M/9.95G [01:00<09:06, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 881M/9.95G [01:01<09:27, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 891M/9.95G [01:01<08:43, 17.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/9.95G [01:02<09:10, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 912M/9.95G [01:03<09:30, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 923M/9.95G [01:03<08:47, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 933M/9.95G [01:04<09:11, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 944M/9.95G [01:05<09:29, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 954M/9.95G [01:05<08:44, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 965M/9.95G [01:06<09:10, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 975M/9.95G [01:07<09:29, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 986M/9.95G [01:07<08:45, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 996M/9.95G [01:08<09:10, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.95G [01:09<09:26, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.02G/9.95G [01:09<08:41, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.03G/9.95G [01:10<09:09, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.04G/9.95G [01:10<09:23, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.05G/9.95G [01:11<08:40, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.06G/9.95G [01:12<09:04, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.95G [01:12<08:26, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.08G/9.95G [01:13<08:54, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.09G/9.95G [01:14<09:14, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.10G/9.95G [01:14<08:31, 17.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.11G/9.95G [01:15<08:57, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.12G/9.95G [01:15<08:20, 17.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.13G/9.95G [01:16<08:49, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.14G/9.95G [01:17<09:11, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.15G/9.95G [01:17<08:36, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.16G/9.95G [01:18<08:56, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.17G/9.95G [01:18<08:19, 17.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.18G/9.95G [01:19<08:44, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.20G/9.95G [01:20<09:05, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.21G/9.95G [01:20<08:23, 17.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.95G [01:21<08:51, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.23G/9.95G [01:22<09:11, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.24G/9.95G [01:22<08:28, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.25G/9.95G [01:23<08:53, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.26G/9.95G [01:24<09:11, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.27G/9.95G [01:25<10:21, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/9.95G [01:25<10:13, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.29G/9.95G [01:26<10:06, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.30G/9.95G [01:27<10:02, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.31G/9.95G [01:27<09:59, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.95G [01:28<09:48, 14.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.33G/9.95G [01:29<09:00, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.34G/9.95G [01:29<09:14, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.35G/9.95G [01:30<09:23, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.36G/9.95G [01:31<08:34, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.37G/9.95G [01:31<08:55, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.38G/9.95G [01:32<09:10, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.39G/9.95G [01:33<09:20, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.41G/9.95G [01:33<08:31, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.42G/9.95G [01:34<08:52, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.43G/9.95G [01:35<09:06, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.44G/9.95G [01:35<09:16, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.45G/9.95G [01:36<08:29, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.46G/9.95G [01:37<08:50, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.47G/9.95G [01:37<09:02, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.95G [01:38<08:27, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.49G/9.95G [01:39<08:39, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.50G/9.95G [01:39<08:56, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.51G/9.95G [01:40<08:21, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.52G/9.95G [01:40<08:37, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.53G/9.95G [01:41<08:51, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.95G [01:42<08:18, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.55G/9.95G [01:42<08:31, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.56G/9.95G [01:43<08:49, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.57G/9.95G [01:44<09:01, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.58G/9.95G [01:44<08:23, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.59G/9.95G [01:45<09:24, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.60G/9.95G [01:46<08:36, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.61G/9.95G [01:46<08:46, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.95G [01:47<08:09, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.64G/9.95G [01:48<08:34, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.65G/9.95G [01:48<08:43, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.66G/9.95G [01:49<08:07, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.95G [01:50<08:30, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.68G/9.95G [01:50<08:43, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.69G/9.95G [01:51<08:04, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.70G/9.95G [01:51<08:27, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.71G/9.95G [01:52<08:38, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.72G/9.95G [01:53<08:03, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/9.95G [01:53<08:22, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.74G/9.95G [01:54<08:37, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.75G/9.95G [01:55<08:01, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.95G [01:55<08:18, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.77G/9.95G [01:56<07:49, 17.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.78G/9.95G [01:57<08:15, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.79G/9.95G [01:57<08:27, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.80G/9.95G [01:58<08:03, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.81G/9.95G [01:58<08:15, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.82G/9.95G [01:59<08:26, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.84G/9.95G [02:00<07:53, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.85G/9.95G [02:00<08:12, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.86G/9.95G [02:01<07:41, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.87G/9.95G [02:02<08:08, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.88G/9.95G [02:02<08:21, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.95G [02:03<07:49, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.90G/9.95G [02:03<08:07, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.91G/9.95G [02:04<08:24, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.92G/9.95G [02:05<07:52, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.93G/9.95G [02:05<08:07, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.94G/9.95G [02:06<07:39, 17.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.95G/9.95G [02:07<07:58, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.96G/9.95G [02:07<08:18, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.97G/9.95G [02:08<07:48, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.98G/9.95G [02:09<08:02, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.99G/9.95G [02:09<08:19, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.00G/9.95G [02:10<08:32, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.01G/9.95G [02:10<07:49, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.02G/9.95G [02:11<08:12, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.03G/9.95G [02:12<07:32, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.04G/9.95G [02:12<08:00, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.06G/9.95G [02:13<08:16, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.07G/9.95G [02:14<07:37, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.08G/9.95G [02:14<07:59, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.09G/9.95G [02:15<08:16, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.10G/9.95G [02:16<07:40, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.11G/9.95G [02:16<07:59, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.12G/9.95G [02:17<08:16, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.13G/9.95G [02:18<08:29, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.14G/9.95G [02:18<08:36, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.15G/9.95G [02:19<08:42, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.16G/9.95G [02:20<08:43, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.17G/9.95G [02:20<07:56, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.18G/9.95G [02:21<08:11, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.19G/9.95G [02:22<08:22, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.20G/9.95G [02:22<07:42, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.21G/9.95G [02:23<08:03, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.22G/9.95G [02:24<08:12, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/9.95G [02:24<07:31, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.24G/9.95G [02:25<07:53, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.25G/9.95G [02:26<09:01, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.26G/9.95G [02:27<08:55, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.28G/9.95G [02:27<08:52, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.29G/9.95G [02:28<08:52, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/9.95G [02:29<08:47, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.31G/9.95G [02:29<08:50, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.32G/9.95G [02:30<09:32, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.33G/9.95G [02:31<09:16, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.34G/9.95G [02:32<08:20, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.35G/9.95G [02:33<09:16, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.36G/9.95G [02:33<09:55, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.37G/9.95G [02:34<10:15, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.38G/9.95G [02:35<09:53, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.39G/9.95G [02:36<10:13, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.40G/9.95G [02:37<09:50, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.41G/9.95G [02:38<10:16, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.42G/9.95G [02:39<09:55, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.43G/9.95G [02:39<10:10, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.44G/9.95G [02:40<10:23, 12.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.45G/9.95G [02:41<09:57, 12.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.46G/9.95G [02:42<10:12, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.47G/9.95G [02:43<09:49, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.49G/9.95G [02:44<10:06, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.50G/9.95G [02:44<09:44, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.51G/9.95G [02:45<10:02, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.52G/9.95G [02:46<09:42, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.53G/9.95G [02:47<09:19, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.54G/9.95G [02:48<09:43, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.55G/9.95G [02:48<09:27, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.56G/9.95G [02:49<09:08, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.57G/9.95G [02:50<08:58, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.58G/9.95G [02:51<08:47, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.59G/9.95G [02:52<09:24, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.60G/9.95G [02:52<09:06, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.61G/9.95G [02:53<08:04, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.62G/9.95G [02:54<08:12, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.63G/9.95G [02:54<08:11, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.64G/9.95G [02:55<08:12, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.65G/9.95G [02:56<08:14, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.66G/9.95G [02:56<07:27, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.67G/9.95G [02:57<07:42, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.68G/9.95G [02:58<07:54, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.69G/9.95G [02:58<07:59, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.71G/9.95G [02:59<08:04, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.72G/9.95G [03:00<08:07, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.73G/9.95G [03:00<08:08, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.74G/9.95G [03:01<08:09, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.75G/9.95G [03:02<07:24, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.76G/9.95G [03:02<07:37, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.77G/9.95G [03:03<07:46, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.78G/9.95G [03:04<08:40, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.79G/9.95G [03:05<08:31, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.80G/9.95G [03:06<09:10, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.81G/9.95G [03:06<08:52, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.95G [03:07<08:39, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.83G/9.95G [03:08<08:29, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.84G/9.95G [03:09<08:25, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.85G/9.95G [03:09<08:18, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.86G/9.95G [03:10<08:12, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.87G/9.95G [03:11<08:08, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.88G/9.95G [03:11<08:09, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.89G/9.95G [03:12<08:04, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.90G/9.95G [03:13<08:04, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.92G/9.95G [03:14<08:42, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.93G/9.95G [03:15<08:40, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.94G/9.95G [03:16<09:13, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.95G/9.95G [03:16<09:33, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.96G/9.95G [03:17<09:08, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.97G/9.95G [03:18<09:28, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.98G/9.95G [03:19<09:03, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.99G/9.95G [03:20<09:24, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.00G/9.95G [03:21<08:57, 12.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.01G/9.95G [03:21<08:38, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.02G/9.95G [03:22<08:24, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.03G/9.95G [03:23<08:16, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.04G/9.95G [03:23<08:06, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.05G/9.95G [03:24<08:02, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.06G/9.95G [03:25<07:59, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.07G/9.95G [03:26<07:56, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.08G/9.95G [03:26<07:53, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.09G/9.95G [03:27<07:52, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.10G/9.95G [03:28<08:34, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.11G/9.95G [03:29<08:19, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.12G/9.95G [03:29<08:11, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.14G/9.95G [03:30<08:03, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.15G/9.95G [03:31<07:57, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.16G/9.95G [03:32<07:53, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.17G/9.95G [03:32<07:50, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.18G/9.95G [03:33<07:48, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.19G/9.95G [03:34<07:46, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.20G/9.95G [03:35<08:29, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.21G/9.95G [03:36<08:58, 12.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.22G/9.95G [03:36<08:36, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.23G/9.95G [03:37<08:19, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.24G/9.95G [03:38<08:45, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.25G/9.95G [03:39<08:30, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.26G/9.95G [03:39<08:13, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.27G/9.95G [03:40<07:59, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.28G/9.95G [03:41<07:14, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/9.95G [03:41<07:22, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.30G/9.95G [03:42<07:22, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.31G/9.95G [03:43<07:24, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.32G/9.95G [03:43<07:25, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.33G/9.95G [03:44<07:27, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.34G/9.95G [03:45<07:23, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.36G/9.95G [03:45<06:47, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.37G/9.95G [03:46<07:01, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.38G/9.95G [03:47<07:08, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.39G/9.95G [03:48<07:14, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.40G/9.95G [03:48<07:14, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.41G/9.95G [03:49<06:38, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.42G/9.95G [03:49<06:52, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.43G/9.95G [03:50<07:02, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.44G/9.95G [03:51<07:08, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.45G/9.95G [03:51<06:31, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.46G/9.95G [03:52<06:46, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.47G/9.95G [03:53<06:56, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.48G/9.95G [03:54<07:03, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.49G/9.95G [03:54<06:26, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/9.95G [03:55<06:45, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.51G/9.95G [03:55<06:52, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.52G/9.95G [03:56<06:19, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.53G/9.95G [03:57<06:36, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.54G/9.95G [03:57<06:48, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.55G/9.95G [03:58<06:15, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.95G [03:59<06:33, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.58G/9.95G [03:59<06:45, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.59G/9.95G [04:00<06:13, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.60G/9.95G [04:01<06:31, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.61G/9.95G [04:01<06:44, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.62G/9.95G [04:02<06:11, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.63G/9.95G [04:02<06:29, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.64G/9.95G [04:03<06:41, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.65G/9.95G [04:04<06:09, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.66G/9.95G [04:04<06:26, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.67G/9.95G [04:05<06:39, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.68G/9.95G [04:06<06:47, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.69G/9.95G [04:06<06:12, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.70G/9.95G [04:07<06:28, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.71G/9.95G [04:08<06:39, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.72G/9.95G [04:08<06:06, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.73G/9.95G [04:09<06:23, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.74G/9.95G [04:10<06:35, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.75G/9.95G [04:10<06:43, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.76G/9.95G [04:11<06:49, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.77G/9.95G [04:12<06:13, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.79G/9.95G [04:12<06:26, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.80G/9.95G [04:13<05:56, 17.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.81G/9.95G [04:14<06:15, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.82G/9.95G [04:14<06:26, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.83G/9.95G [04:15<06:36, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.84G/9.95G [04:16<06:45, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.85G/9.95G [04:17<07:26, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.86G/9.95G [04:17<07:17, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.87G/9.95G [04:18<07:11, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.88G/9.95G [04:19<07:05, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.89G/9.95G [04:19<07:00, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.90G/9.95G [04:20<06:58, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.91G/9.95G [04:21<06:56, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/9.95G [04:21<06:15, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.93G/9.95G [04:22<06:26, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.94G/9.95G [04:23<06:33, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.95G/9.95G [04:24<06:37, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.96G/9.95G [04:24<06:40, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.97G/9.95G [04:25<06:42, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.98G/9.95G [04:25<06:05, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.00G/9.95G [04:26<06:16, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.01G/9.95G [04:27<06:25, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.02G/9.95G [04:28<06:30, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.03G/9.95G [04:28<05:56, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.04G/9.95G [04:29<06:10, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.05G/9.95G [04:30<06:19, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.06G/9.95G [04:31<07:06, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.07G/9.95G [04:31<06:59, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.08G/9.95G [04:32<07:30, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.09G/9.95G [04:33<07:54, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.10G/9.95G [04:34<08:11, 11.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/9.95G [04:35<08:22, 11.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.12G/9.95G [04:36<08:29, 11.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.13G/9.95G [04:37<08:33, 11.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.14G/9.95G [04:38<08:36, 11.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.15G/9.95G [04:39<08:38, 11.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.16G/9.95G [04:40<08:05, 11.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.17G/9.95G [04:41<08:11, 11.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.18G/9.95G [04:41<08:20, 11.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.19G/9.95G [04:42<08:24, 11.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.20G/9.95G [04:43<07:53, 12.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.22G/9.95G [04:44<08:05, 11.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.23G/9.95G [04:45<08:14, 11.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.24G/9.95G [04:46<07:46, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.25G/9.95G [04:47<07:57, 11.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.26G/9.95G [04:48<08:07, 11.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.27G/9.95G [04:48<07:41, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.28G/9.95G [04:49<07:53, 12.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.29G/9.95G [04:50<07:32, 12.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.95G [04:51<07:43, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.31G/9.95G [04:52<07:21, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.32G/9.95G [04:53<07:36, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.33G/9.95G [04:53<07:14, 12.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.34G/9.95G [04:54<06:57, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.35G/9.95G [04:55<06:49, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.36G/9.95G [04:55<06:05, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.37G/9.95G [04:56<06:09, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.38G/9.95G [04:57<06:10, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.39G/9.95G [04:57<05:38, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.40G/9.95G [04:58<05:51, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.41G/9.95G [04:59<06:33, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/9.95G [05:00<06:30, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.44G/9.95G [05:00<06:24, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.45G/9.95G [05:01<06:23, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.46G/9.95G [05:02<06:56, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.47G/9.95G [05:03<06:49, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.48G/9.95G [05:04<07:07, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.49G/9.95G [05:04<06:52, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.50G/9.95G [05:05<06:41, 13.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.51G/9.95G [05:06<06:34, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.52G/9.95G [05:07<06:57, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.53G/9.95G [05:07<06:45, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.54G/9.95G [05:08<06:35, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.55G/9.95G [05:09<06:27, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.56G/9.95G [05:10<06:26, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.57G/9.95G [05:11<06:44, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.58G/9.95G [05:11<06:38, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.59G/9.95G [05:12<06:27, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.60G/9.95G [05:13<06:21, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.61G/9.95G [05:13<06:19, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.62G/9.95G [05:14<06:39, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.63G/9.95G [05:15<06:34, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.65G/9.95G [05:16<06:24, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.66G/9.95G [05:17<06:17, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.67G/9.95G [05:17<06:13, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.68G/9.95G [05:18<06:42, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.69G/9.95G [05:19<06:29, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.70G/9.95G [05:19<05:45, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.71G/9.95G [05:20<05:49, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.72G/9.95G [05:21<05:53, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.73G/9.95G [05:22<05:52, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.74G/9.95G [05:23<06:28, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.75G/9.95G [05:23<06:19, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.76G/9.95G [05:24<06:43, 12.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.77G/9.95G [05:25<06:29, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.78G/9.95G [05:26<06:18, 13.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.79G/9.95G [05:26<06:11, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/9.95G [05:27<06:04, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.81G/9.95G [05:28<06:00, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.82G/9.95G [05:28<05:55, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.83G/9.95G [05:29<05:52, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.84G/9.95G [05:30<05:19, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.85G/9.95G [05:30<05:27, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.87G/9.95G [05:31<05:31, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.88G/9.95G [05:32<05:05, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.89G/9.95G [05:32<05:16, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.95G [05:33<05:23, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.91G/9.95G [05:34<04:59, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.92G/9.95G [05:34<05:12, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.93G/9.95G [05:35<05:21, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.94G/9.95G [05:35<04:55, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.95G/9.95G [05:36<05:09, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.96G/9.95G [05:37<05:18, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.97G/9.95G [05:38<05:24, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.98G/9.95G [05:38<04:56, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.99G/9.95G [05:39<05:08, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.00G/9.95G [05:40<05:17, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.01G/9.95G [05:40<04:51, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.02G/9.95G [05:41<05:04, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.03G/9.95G [05:41<05:13, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.04G/9.95G [05:42<04:48, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.05G/9.95G [05:43<05:01, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.06G/9.95G [05:43<05:10, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.08G/9.95G [05:44<04:45, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.09G/9.95G [05:45<05:32, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.10G/9.95G [05:46<05:30, 14.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.11G/9.95G [05:46<05:32, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/9.95G [05:47<05:30, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.13G/9.95G [05:48<05:29, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.14G/9.95G [05:48<05:28, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.15G/9.95G [05:49<05:28, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.16G/9.95G [05:50<04:56, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.17G/9.95G [05:50<05:05, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.18G/9.95G [05:51<05:11, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.19G/9.95G [05:52<04:43, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.20G/9.95G [05:52<04:56, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.21G/9.95G [05:53<05:03, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.22G/9.95G [05:54<04:38, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.23G/9.95G [05:54<04:51, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.95G [05:55<04:59, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.25G/9.95G [05:55<04:34, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.26G/9.95G [05:56<04:47, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.27G/9.95G [05:57<04:57, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.28G/9.95G [05:57<04:32, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.30G/9.95G [05:58<05:18, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.31G/9.95G [05:59<05:46, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.32G/9.95G [06:00<05:38, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.33G/9.95G [06:01<05:31, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.34G/9.95G [06:01<05:26, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.35G/9.95G [06:02<05:23, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.36G/9.95G [06:03<05:19, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.37G/9.95G [06:04<05:17, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.38G/9.95G [06:04<05:15, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.39G/9.95G [06:05<05:13, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.40G/9.95G [06:06<05:12, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.41G/9.95G [06:06<05:14, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.42G/9.95G [06:07<05:10, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.43G/9.95G [06:08<05:09, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.44G/9.95G [06:09<05:08, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.45G/9.95G [06:09<05:07, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.46G/9.95G [06:10<05:07, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.47G/9.95G [06:11<05:06, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.48G/9.95G [06:11<05:05, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.49G/9.95G [06:12<05:04, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.95G [06:13<05:03, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.52G/9.95G [06:13<04:35, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.53G/9.95G [06:14<04:42, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.54G/9.95G [06:15<04:48, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.55G/9.95G [06:16<04:51, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.56G/9.95G [06:16<04:53, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.57G/9.95G [06:17<04:26, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.58G/9.95G [06:17<04:37, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.59G/9.95G [06:18<04:41, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.60G/9.95G [06:19<04:17, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.61G/9.95G [06:19<04:28, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.62G/9.95G [06:20<04:36, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.63G/9.95G [06:21<04:42, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.64G/9.95G [06:21<04:18, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.65G/9.95G [06:22<04:27, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.66G/9.95G [06:23<04:34, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.67G/9.95G [06:23<04:11, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.68G/9.95G [06:24<04:22, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.69G/9.95G [06:25<05:00, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.70G/9.95G [06:26<04:56, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.71G/9.95G [06:26<04:54, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.73G/9.95G [06:27<04:24, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.74G/9.95G [06:28<04:31, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.75G/9.95G [06:28<04:35, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.76G/9.95G [06:29<04:10, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.77G/9.95G [06:29<04:20, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.78G/9.95G [06:30<04:28, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.79G/9.95G [06:31<04:33, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.80G/9.95G [06:31<04:13, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.81G/9.95G [06:32<04:45, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.82G/9.95G [06:33<04:44, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.83G/9.95G [06:34<04:42, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.84G/9.95G [06:34<04:41, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.85G/9.95G [06:36<05:33, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.86G/9.95G [06:36<05:18, 12.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.87G/9.95G [06:37<05:32, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.88G/9.95G [06:38<05:41, 11.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.89G/9.95G [06:39<05:22, 12.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.90G/9.95G [06:40<05:34, 12.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.95G [06:41<05:16, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.92G/9.95G [06:42<05:30, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.93G/9.95G [06:42<05:13, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.95G/9.95G [06:43<05:22, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.96G/9.95G [06:44<05:12, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.97G/9.95G [06:45<05:01, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.95G [06:46<05:16, 12.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.99G/9.95G [06:46<05:02, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.00G/9.95G [06:47<04:55, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.01G/9.95G [06:48<05:11, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.02G/9.95G [06:49<04:57, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.03G/9.95G [06:50<04:52, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.04G/9.95G [06:50<05:05, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.05G/9.95G [06:51<04:57, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.06G/9.95G [06:52<04:48, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.07G/9.95G [06:53<05:00, 12.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.08G/9.95G [06:54<04:50, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.09G/9.95G [06:54<04:42, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.10G/9.95G [06:55<04:32, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.11G/9.95G [06:56<04:11, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.12G/9.95G [06:56<04:30, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.13G/9.95G [06:57<04:07, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.14G/9.95G [06:58<04:06, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.16G/9.95G [06:58<04:09, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.17G/9.95G [06:59<03:51, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.18G/9.95G [07:00<03:56, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.19G/9.95G [07:00<04:00, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.20G/9.95G [07:01<03:44, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.21G/9.95G [07:01<03:49, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.22G/9.95G [07:02<03:56, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.23G/9.95G [07:03<03:41, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.24G/9.95G [07:03<03:46, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.25G/9.95G [07:04<03:34, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.26G/9.95G [07:05<03:44, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.27G/9.95G [07:05<03:48, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.28G/9.95G [07:06<03:34, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.29G/9.95G [07:06<03:40, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.30G/9.95G [07:07<03:48, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.95G [07:08<03:34, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.32G/9.95G [07:08<03:41, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.33G/9.95G [07:09<03:27, 17.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.34G/9.95G [07:10<03:34, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.35G/9.95G [07:10<03:43, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.36G/9.95G [07:11<03:30, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.38G/9.95G [07:12<03:35, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.39G/9.95G [07:12<03:25, 17.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.40G/9.95G [07:13<03:35, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.41G/9.95G [07:13<03:42, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.42G/9.95G [07:14<03:24, 17.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.43G/9.95G [07:15<03:35, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.44G/9.95G [07:15<03:42, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.45G/9.95G [07:16<03:24, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.46G/9.95G [07:17<03:34, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.47G/9.95G [07:17<03:40, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.48G/9.95G [07:18<03:22, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.49G/9.95G [07:19<03:32, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.50G/9.95G [07:19<03:16, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.51G/9.95G [07:20<03:27, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.52G/9.95G [07:20<03:35, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.53G/9.95G [07:21<03:18, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.54G/9.95G [07:22<03:28, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.55G/9.95G [07:22<03:34, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.56G/9.95G [07:23<03:16, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.57G/9.95G [07:24<03:26, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.59G/9.95G [07:24<03:33, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.60G/9.95G [07:25<03:17, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.61G/9.95G [07:26<03:24, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.62G/9.95G [07:26<03:31, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.63G/9.95G [07:27<03:14, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.64G/9.95G [07:27<03:22, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.65G/9.95G [07:28<03:07, 17.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.66G/9.95G [07:29<03:18, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.67G/9.95G [07:29<03:25, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.68G/9.95G [07:30<03:12, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.69G/9.95G [07:31<03:18, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.70G/9.95G [07:31<03:25, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.71G/9.95G [07:32<03:11, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.72G/9.95G [07:32<03:17, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.73G/9.95G [07:33<03:22, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.74G/9.95G [07:34<03:09, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.75G/9.95G [07:34<03:14, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.76G/9.95G [07:35<03:03, 17.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.77G/9.95G [07:36<03:09, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.78G/9.95G [07:36<03:00, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.79G/9.95G [07:37<03:09, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.81G/9.95G [07:38<03:13, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.82G/9.95G [07:38<03:19, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.83G/9.95G [07:39<03:03, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.84G/9.95G [07:39<03:11, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.85G/9.95G [07:40<03:16, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.86G/9.95G [07:41<03:21, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.87G/9.95G [07:41<03:03, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.88G/9.95G [07:42<03:11, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.89G/9.95G [07:43<03:16, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.90G/9.95G [07:44<03:19, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.91G/9.95G [07:44<03:01, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.92G/9.95G [07:45<03:08, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.93G/9.95G [07:45<02:54, 17.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.94G/9.95G [07:46<03:02, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.95G/9.95G [07:47<03:08, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.96G/9.95G [07:47<02:53, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.97G/9.95G [07:48<03:02, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.98G/9.95G [07:48<02:48, 17.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.99G/9.95G [07:49<02:58, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.00G/9.95G [07:50<03:04, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.01G/9.95G [07:50<02:49, 17.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.03G/9.95G [07:51<02:58, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.04G/9.95G [07:51<02:45, 17.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.05G/9.95G [07:52<02:55, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.06G/9.95G [07:53<02:43, 17.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.07G/9.95G [07:53<02:50, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.08G/9.95G [07:54<02:58, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.09G/9.95G [07:55<02:45, 17.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.10G/9.95G [07:55<02:53, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.11G/9.95G [07:56<02:58, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.12G/9.95G [07:57<02:44, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.13G/9.95G [07:57<02:52, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.14G/9.95G [07:58<02:57, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.15G/9.95G [07:58<02:43, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.16G/9.95G [07:59<02:50, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.17G/9.95G [08:00<02:38, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.18G/9.95G [08:00<02:47, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.19G/9.95G [08:01<02:52, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.20G/9.95G [08:02<02:39, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.21G/9.95G [08:02<02:46, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.22G/9.95G [08:03<02:51, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.24G/9.95G [08:03<02:38, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.25G/9.95G [08:04<02:45, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.26G/9.95G [08:05<03:08, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.27G/9.95G [08:06<03:06, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.28G/9.95G [08:06<02:47, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.29G/9.95G [08:07<02:51, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.30G/9.95G [08:08<02:53, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.31G/9.95G [08:08<02:55, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.32G/9.95G [08:09<02:39, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.33G/9.95G [08:10<02:44, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.34G/9.95G [08:10<02:47, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.35G/9.95G [08:11<02:50, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.36G/9.95G [08:12<02:52, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.37G/9.95G [08:12<02:36, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.95G [08:13<02:41, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.39G/9.95G [08:14<02:44, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.40G/9.95G [08:14<02:30, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.41G/9.95G [08:15<02:37, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.42G/9.95G [08:16<02:41, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.43G/9.95G [08:16<02:44, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.44G/9.95G [08:17<02:30, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.46G/9.95G [08:18<03:08, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.47G/9.95G [08:19<03:04, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.48G/9.95G [08:20<02:57, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.49G/9.95G [08:20<02:58, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.50G/9.95G [08:21<03:06, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.51G/9.95G [08:22<03:01, 13.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.52G/9.95G [08:23<02:55, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.53G/9.95G [08:23<02:52, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.54G/9.95G [08:24<02:49, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.55G/9.95G [08:25<02:47, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.56G/9.95G [08:25<02:45, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.57G/9.95G [08:26<02:44, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.58G/9.95G [08:27<02:44, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.59G/9.95G [08:28<02:43, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.60G/9.95G [08:28<02:40, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.61G/9.95G [08:29<02:40, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.62G/9.95G [08:30<02:39, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.63G/9.95G [08:31<02:50, 13.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.64G/9.95G [08:31<02:49, 13.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.65G/9.95G [08:32<02:41, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.67G/9.95G [08:33<02:28, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.68G/9.95G [08:34<02:40, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.69G/9.95G [08:34<02:28, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.70G/9.95G [08:35<02:30, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.71G/9.95G [08:36<02:31, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.72G/9.95G [08:36<02:33, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.73G/9.95G [08:37<02:46, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.74G/9.95G [08:38<02:41, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.75G/9.95G [08:39<02:52, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.76G/9.95G [08:40<02:45, 13.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.77G/9.95G [08:40<02:39, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.95G [08:41<02:36, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.79G/9.95G [08:42<02:32, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.80G/9.95G [08:43<02:30, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.81G/9.95G [08:43<02:29, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.82G/9.95G [08:44<02:27, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.83G/9.95G [08:44<02:14, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.84G/9.95G [08:45<02:15, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.85G/9.95G [08:46<02:16, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.86G/9.95G [08:47<02:18, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.87G/9.95G [08:47<02:18, 15.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.89G/9.95G [08:48<02:18, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.90G/9.95G [08:49<02:18, 14.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.91G/9.95G [08:49<02:06, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.92G/9.95G [08:50<02:08, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.93G/9.95G [08:51<02:11, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.94G/9.95G [08:52<02:24, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.95G/9.95G [08:52<02:23, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.96G/9.95G [08:53<02:32, 13.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.97G/9.95G [08:54<02:28, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.98G/9.95G [08:55<02:23, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.99G/9.95G [08:55<02:20, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.00G/9.95G [08:56<02:17, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.01G/9.95G [08:57<02:15, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.02G/9.95G [08:58<02:13, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.03G/9.95G [08:58<02:12, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.04G/9.95G [08:59<02:08, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.05G/9.95G [09:00<01:59, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.06G/9.95G [09:00<02:02, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.07G/9.95G [09:01<02:04, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.08G/9.95G [09:02<02:05, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.10G/9.95G [09:02<02:03, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.11G/9.95G [09:03<01:53, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.12G/9.95G [09:04<01:57, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.13G/9.95G [09:04<01:58, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.14G/9.95G [09:05<01:59, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.15G/9.95G [09:06<01:48, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.16G/9.95G [09:06<01:52, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.17G/9.95G [09:07<01:54, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.18G/9.95G [09:07<01:44, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.19G/9.95G [09:08<02:00, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.20G/9.95G [09:09<02:00, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.21G/9.95G [09:10<01:48, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.22G/9.95G [09:10<01:50, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.23G/9.95G [09:11<01:52, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.24G/9.95G [09:12<01:42, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.25G/9.95G [09:12<01:46, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.26G/9.95G [09:13<01:47, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.27G/9.95G [09:14<01:38, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.28G/9.95G [09:14<01:42, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.29G/9.95G [09:15<01:45, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.30G/9.95G [09:15<01:36, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.32G/9.95G [09:16<01:40, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.33G/9.95G [09:17<01:42, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.34G/9.95G [09:17<01:34, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.35G/9.95G [09:18<01:38, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.36G/9.95G [09:19<01:40, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.37G/9.95G [09:19<01:33, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.38G/9.95G [09:20<01:36, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.39G/9.95G [09:21<01:38, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.40G/9.95G [09:21<01:30, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.41G/9.95G [09:22<01:34, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.42G/9.95G [09:22<01:27, 17.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.43G/9.95G [09:23<01:31, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.44G/9.95G [09:24<01:34, 16.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.45G/9.95G [09:24<01:27, 17.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.46G/9.95G [09:25<01:31, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.47G/9.95G [09:26<01:33, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.48G/9.95G [09:26<01:26, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.49G/9.95G [09:27<01:29, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.50G/9.95G [09:28<01:31, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.51G/9.95G [09:28<01:24, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.52G/9.95G [09:29<01:27, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.54G/9.95G [09:30<01:29, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.55G/9.95G [09:30<01:22, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.56G/9.95G [09:31<01:25, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.57G/9.95G [09:32<01:27, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.58G/9.95G [09:32<01:21, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.59G/9.95G [09:33<01:23, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.60G/9.95G [09:34<01:34, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.61G/9.95G [09:34<01:33, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.62G/9.95G [09:35<01:32, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.63G/9.95G [09:36<01:31, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.64G/9.95G [09:37<01:30, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.65G/9.95G [09:37<01:29, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.66G/9.95G [09:38<01:28, 14.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.67G/9.95G [09:38<01:19, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.68G/9.95G [09:39<01:21, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.69G/9.95G [09:40<01:22, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.70G/9.95G [09:41<01:21, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.71G/9.95G [09:41<01:15, 16.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.72G/9.95G [09:42<01:17, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.73G/9.95G [09:43<01:18, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.75G/9.95G [09:43<01:19, 15.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.76G/9.95G [09:44<01:11, 16.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.77G/9.95G [09:44<01:14, 15.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.78G/9.95G [09:45<01:15, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.79G/9.95G [09:46<01:09, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.80G/9.95G [09:46<01:11, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.81G/9.95G [09:47<01:13, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.82G/9.95G [09:48<01:06, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.83G/9.95G [09:48<01:08, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.84G/9.95G [09:49<01:10, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.85G/9.95G [09:50<01:11, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.86G/9.95G [09:50<01:04, 16.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.87G/9.95G [09:51<01:06, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.88G/9.95G [09:52<01:08, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.89G/9.95G [09:52<01:02, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.90G/9.95G [09:53<01:04, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.91G/9.95G [09:54<01:05, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.92G/9.95G [09:54<01:00, 17.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.93G/9.95G [09:55<01:02, 16.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.94G/9.95G [09:56<01:10, 14.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.95G/9.95G [09:56<01:09, 14.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.97G/9.95G [09:57<01:14, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.98G/9.95G [09:58<01:11, 13.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.99G/9.95G [09:59<01:09, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.00G/9.95G [10:00<01:09, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.01G/9.95G [10:01<01:12, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.02G/9.95G [10:01<01:15, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.03G/9.95G [10:02<01:17, 11.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.04G/9.95G [10:03<01:12, 12.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.05G/9.95G [10:04<01:14, 12.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.06G/9.95G [10:05<01:21, 10.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.07G/9.95G [10:06<01:26, 10.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.08G/9.95G [10:07<01:24, 10.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.09G/9.95G [10:09<01:26, 9.92MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.10G/9.95G [10:10<01:23, 10.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.11G/9.95G [10:11<01:20, 10.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.12G/9.95G [10:11<01:18, 10.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.13G/9.95G [10:13<01:20, 10.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.14G/9.95G [10:14<01:18, 10.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.15G/9.95G [10:15<01:16, 10.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.16G/9.95G [10:16<01:14, 10.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.95G [10:17<01:16, 10.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.19G/9.95G [10:18<01:13, 10.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.20G/9.95G [10:19<01:11, 10.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.21G/9.95G [10:20<01:09, 10.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.22G/9.95G [10:20<01:07, 10.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.23G/9.95G [10:21<01:06, 10.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.24G/9.95G [10:22<01:05, 11.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.25G/9.95G [10:23<01:00, 11.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.26G/9.95G [10:24<00:59, 11.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.27G/9.95G [10:25<00:55, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.28G/9.95G [10:26<00:52, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.29G/9.95G [10:26<00:53, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.95G [10:27<00:54, 11.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.31G/9.95G [10:28<00:54, 11.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.32G/9.95G [10:29<00:50, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.33G/9.95G [10:30<00:51, 12.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.34G/9.95G [10:31<00:48, 12.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.35G/9.95G [10:31<00:45, 13.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.36G/9.95G [10:32<00:43, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.37G/9.95G [10:33<00:41, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.38G/9.95G [10:34<00:40, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.40G/9.95G [10:35<00:45, 12.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.41G/9.95G [10:36<00:46, 11.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.42G/9.95G [10:36<00:43, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.43G/9.95G [10:37<00:43, 11.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.44G/9.95G [10:38<00:43, 11.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.45G/9.95G [10:39<00:41, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.46G/9.95G [10:40<00:40, 12.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.47G/9.95G [10:41<00:41, 11.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.48G/9.95G [10:42<00:37, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.49G/9.95G [10:43<00:38, 12.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.50G/9.95G [10:43<00:36, 12.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.51G/9.95G [10:44<00:36, 12.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.52G/9.95G [10:45<00:33, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.53G/9.95G [10:46<00:34, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.54G/9.95G [10:47<00:32, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.55G/9.95G [10:48<00:32, 12.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.56G/9.95G [10:48<00:30, 12.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.57G/9.95G [10:49<00:30, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.58G/9.95G [10:50<00:28, 12.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.59G/9.95G [10:51<00:28, 12.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.60G/9.95G [10:52<00:26, 13.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.62G/9.95G [10:52<00:25, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.63G/9.95G [10:53<00:23, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.64G/9.95G [10:54<00:22, 13.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.65G/9.95G [10:55<00:21, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.66G/9.95G [10:55<00:20, 14.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.67G/9.95G [10:56<00:19, 14.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.68G/9.95G [10:56<00:17, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.69G/9.95G [10:57<00:16, 15.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.70G/9.95G [10:58<00:16, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.71G/9.95G [10:58<00:14, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.72G/9.95G [10:59<00:14, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.73G/9.95G [11:00<00:14, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.74G/9.95G [11:01<00:13, 15.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.75G/9.95G [11:01<00:12, 16.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.76G/9.95G [11:02<00:11, 16.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.77G/9.95G [11:02<00:10, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.78G/9.95G [11:03<00:10, 16.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.79G/9.95G [11:04<00:09, 15.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.80G/9.95G [11:04<00:09, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.81G/9.95G [11:05<00:08, 16.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.83G/9.95G [11:06<00:07, 16.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.84G/9.95G [11:06<00:07, 15.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.85G/9.95G [11:07<00:07, 13.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.86G/9.95G [11:08<00:06, 15.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.87G/9.95G [11:08<00:05, 15.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.88G/9.95G [11:09<00:04, 15.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.89G/9.95G [11:10<00:04, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.90G/9.95G [11:11<00:03, 14.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.91G/9.95G [11:11<00:02, 14.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.92G/9.95G [11:12<00:02, 13.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.93G/9.95G [11:13<00:01, 13.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.94G/9.95G [11:14<00:00, 14.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.95G/9.95G [11:14<00:00, 14.7MB/s]\n",
            "Downloading shards:  50% 1/2 [11:16<11:16, 676.50s/it]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/4.48G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   0% 21.0M/4.48G [00:00<00:44, 101MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 52.4M/4.48G [00:00<00:28, 155MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 94.4M/4.48G [00:00<00:20, 218MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 136M/4.48G [00:00<00:16, 266MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 178M/4.48G [00:00<00:14, 296MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 220M/4.48G [00:00<00:13, 314MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 262M/4.48G [00:00<00:12, 333MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 304M/4.48G [00:01<00:13, 311MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 346M/4.48G [00:01<00:13, 302MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 377M/4.48G [00:01<00:14, 282MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 409M/4.48G [00:01<00:14, 283MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 440M/4.48G [00:01<00:14, 288MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 472M/4.48G [00:01<00:14, 274MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 503M/4.48G [00:01<00:14, 278MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 545M/4.48G [00:01<00:13, 293MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 577M/4.48G [00:02<00:13, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 608M/4.48G [00:02<00:13, 293MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 650M/4.48G [00:02<00:12, 308MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 682M/4.48G [00:02<00:12, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 713M/4.48G [00:02<00:12, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 744M/4.48G [00:02<00:13, 278MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 776M/4.48G [00:02<00:13, 277MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 807M/4.48G [00:02<00:13, 267MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 839M/4.48G [00:03<00:13, 263MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 870M/4.48G [00:03<00:13, 260MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 902M/4.48G [00:03<00:14, 253MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 933M/4.48G [00:03<00:13, 258MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 965M/4.48G [00:03<00:14, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 996M/4.48G [00:03<00:13, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 1.03G/4.48G [00:03<00:13, 256MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 1.06G/4.48G [00:03<00:13, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 1.09G/4.48G [00:04<00:13, 252MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 1.12G/4.48G [00:04<00:13, 251MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 1.15G/4.48G [00:04<00:13, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 1.18G/4.48G [00:04<00:13, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 1.22G/4.48G [00:04<00:12, 257MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 1.25G/4.48G [00:04<00:12, 253MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.28G/4.48G [00:04<00:12, 254MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.31G/4.48G [00:04<00:12, 251MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.34G/4.48G [00:05<00:12, 251MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.37G/4.48G [00:05<00:12, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.41G/4.48G [00:05<00:12, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.44G/4.48G [00:05<00:12, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.47G/4.48G [00:05<00:12, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.50G/4.48G [00:05<00:12, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.53G/4.48G [00:05<00:11, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.56G/4.48G [00:05<00:11, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.59G/4.48G [00:06<00:11, 259MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.63G/4.48G [00:06<00:16, 176MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.66G/4.48G [00:06<00:16, 170MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.69G/4.48G [00:06<00:15, 182MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.72G/4.48G [00:06<00:15, 183MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.74G/4.48G [00:06<00:15, 182MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.76G/4.48G [00:07<00:15, 174MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.79G/4.48G [00:07<00:14, 190MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.81G/4.48G [00:07<00:14, 178MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.85G/4.48G [00:07<00:13, 195MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.88G/4.48G [00:08<00:47, 54.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.92G/4.48G [00:09<00:31, 82.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.97G/4.48G [00:09<00:20, 122MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 2.00G/4.48G [00:09<00:17, 141MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 2.03G/4.48G [00:09<00:16, 151MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 2.07G/4.48G [00:09<00:14, 170MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 2.10G/4.48G [00:09<00:12, 194MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 2.13G/4.48G [00:09<00:11, 202MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 2.16G/4.48G [00:09<00:10, 211MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 2.19G/4.48G [00:10<00:10, 220MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 2.22G/4.48G [00:10<00:10, 221MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 2.25G/4.48G [00:10<00:09, 232MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 2.29G/4.48G [00:10<00:08, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 2.32G/4.48G [00:10<00:08, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 2.35G/4.48G [00:10<00:08, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 2.38G/4.48G [00:10<00:08, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 2.41G/4.48G [00:10<00:08, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 2.44G/4.48G [00:11<00:08, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 2.47G/4.48G [00:11<00:08, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 2.51G/4.48G [00:11<00:07, 257MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.54G/4.48G [00:11<00:07, 252MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.57G/4.48G [00:11<00:07, 256MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.60G/4.48G [00:11<00:07, 266MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.63G/4.48G [00:11<00:07, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.66G/4.48G [00:11<00:07, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.69G/4.48G [00:12<00:06, 256MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.73G/4.48G [00:12<00:07, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.76G/4.48G [00:12<00:06, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.79G/4.48G [00:12<00:06, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.82G/4.48G [00:12<00:07, 236MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.85G/4.48G [00:12<00:07, 232MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.88G/4.48G [00:12<00:06, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.92G/4.48G [00:12<00:06, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.95G/4.48G [00:13<00:06, 230MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.98G/4.48G [00:13<00:06, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 3.01G/4.48G [00:13<00:06, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 3.05G/4.48G [00:13<00:05, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 3.09G/4.48G [00:13<00:05, 267MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 3.14G/4.48G [00:13<00:04, 291MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 3.18G/4.48G [00:13<00:04, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 3.22G/4.48G [00:14<00:03, 320MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 3.26G/4.48G [00:14<00:04, 284MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 3.29G/4.48G [00:14<00:04, 272MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 3.32G/4.48G [00:14<00:04, 267MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 3.36G/4.48G [00:14<00:04, 252MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 3.39G/4.48G [00:14<00:04, 262MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 3.42G/4.48G [00:14<00:03, 275MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 3.45G/4.48G [00:14<00:03, 276MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 3.48G/4.48G [00:15<00:03, 277MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 3.51G/4.48G [00:15<00:06, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 3.54G/4.48G [00:15<00:06, 146MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 3.59G/4.48G [00:15<00:04, 184MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 3.62G/4.48G [00:15<00:04, 206MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 3.65G/4.48G [00:16<00:03, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 3.68G/4.48G [00:18<00:19, 40.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 3.71G/4.48G [00:18<00:14, 53.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 3.73G/4.48G [00:18<00:11, 63.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 3.75G/4.48G [00:18<00:09, 74.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 3.77G/4.48G [00:18<00:08, 87.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 3.80G/4.48G [00:19<00:08, 80.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 3.83G/4.48G [00:19<00:06, 106MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.86G/4.48G [00:19<00:04, 136MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.90G/4.48G [00:19<00:03, 178MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.94G/4.48G [00:19<00:02, 211MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.98G/4.48G [00:19<00:02, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 4.02G/4.48G [00:19<00:01, 252MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 4.05G/4.48G [00:20<00:01, 266MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 4.09G/4.48G [00:20<00:01, 296MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 4.13G/4.48G [00:20<00:01, 282MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 4.16G/4.48G [00:20<00:01, 239MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 4.19G/4.48G [00:20<00:01, 214MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 4.23G/4.48G [00:20<00:01, 207MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 4.26G/4.48G [00:21<00:01, 196MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 4.29G/4.48G [00:21<00:01, 154MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 4.31G/4.48G [00:24<00:05, 30.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 4.33G/4.48G [00:24<00:04, 37.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 4.35G/4.48G [00:24<00:02, 47.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 4.38G/4.48G [00:24<00:01, 67.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 4.42G/4.48G [00:24<00:00, 100MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 4.46G/4.48G [00:24<00:00, 112MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 4.48G/4.48G [00:24<00:00, 180MB/s]\n",
            "Downloading shards: 100% 2/2 [11:41<00:00, 350.90s/it]\n",
            "Loading checkpoint shards:   0% 0/2 [00:00<?, ?it/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers bitsandbytes accelerate -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgwTXHyvtxfg",
        "outputId": "ada29628-8768-44e0-a53f-d186726c1863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXGrUFZ1sc0o",
        "outputId": "5893bcb5-13d7-4595-f45b-a442b417cd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: list down all hair products\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Chaabi_Assignment_CSV_Chat/COLAB/main.py\", line 36, in <module>\n",
            "    context_list = qdrant_client.search(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/qdrant_client/qdrant_client.py\", line 324, in search\n",
            "    return self._client.search(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/qdrant_client/local/qdrant_local.py\", line 162, in search\n",
            "    collection = self._get_collection(collection_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/qdrant_client/local/qdrant_local.py\", line 121, in _get_collection\n",
            "    raise ValueError(f\"Collection {collection_name} not found\")\n",
            "ValueError: Collection test_collection not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p2OvNNN0tnJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}